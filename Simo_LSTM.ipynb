{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Simo_LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sw7si03PAkTd"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09J8jgZebhqm"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from typing import Tuple\n",
        "from torch import Tensor\n",
        "import math\n",
        "import torch.nn as nn\n",
        "import os\n",
        "import time\n",
        "import numpy\n",
        "\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence"
      ],
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No0DUNuZAjTv"
      },
      "source": [
        "Corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCkC1DCkZo5I"
      },
      "source": [
        "class Dictionary(object):\n",
        "    def __init__(self):\n",
        "        self.word2idx = {} # word: index\n",
        "        self.idx2word = [] # position(index): word\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2idx:\n",
        "            self.idx2word.append(word)\n",
        "            self.word2idx[word] = len(self.idx2word) - 1\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx2word)\n",
        "\n",
        "class Corpus(object):\n",
        "    def __init__(self, path):\n",
        "        self.dictionary = Dictionary()\n",
        "        self.dictionary.add_word('<pad>')\n",
        "        self.train = self.tokenize(os.path.join(path, 'ptb.train.txt'))\n",
        "        self.valid = self.tokenize(os.path.join(path, 'ptb.valid.txt'))\n",
        "        self.test = self.tokenize(os.path.join(path, 'ptb.test.txt'))\n",
        "\n",
        "    def tokenize(self, path):\n",
        "        assert os.path.exists(path)\n",
        "        # Add words to the dictionary\n",
        "        with open(path, 'r') as f:\n",
        "            tokens = 0\n",
        "            for line in f:\n",
        "                # line to list of token + eos\n",
        "                words = line.split() + ['<eos>']\n",
        "                tokens += len(words)\n",
        "                for word in words:\n",
        "                    self.dictionary.add_word(word)\n",
        "\n",
        "        # Tokenize file content\n",
        "        with open(path, 'r') as f:\n",
        "            sentences = []\n",
        "            token = 0\n",
        "            for line in f:\n",
        "                temp = []\n",
        "                words = line.split() + ['<eos>']\n",
        "                for word in words:\n",
        "                    temp.append(self.dictionary.word2idx[word])\n",
        "                sentences.append(torch.LongTensor(temp).to('cuda'))\n",
        "\n",
        "        return sentences"
      ],
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrIw3hantgan"
      },
      "source": [
        "GRU Cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E71AHyd9thuF"
      },
      "source": [
        "class MyGru(nn.Module):\n",
        "    def __init__(self, input_size: int, hidden_size: int):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        super(MyGru, self).__init__()\n",
        "\n",
        "        # Reset gate:\n",
        "        self.reset_ht = nn.Parameter(torch.Tensor(input_size, hidden_size))\n",
        "        self.reset_xt = nn.Parameter(torch.Tensor(input_size, hidden_size))\n",
        "        self.reset_bias = nn.Parameter(torch.Tensor(hidden_size))\n",
        "\n",
        "        # Update gate:\n",
        "        self.update_ht = nn.Parameter(torch.Tensor(input_size, hidden_size))\n",
        "        self.update_xt = nn.Parameter(torch.Tensor(input_size, hidden_size))\n",
        "        self.update_bias = nn.Parameter(torch.Tensor(hidden_size))\n",
        "\n",
        "        # Output gate:\n",
        "        self.output_qt = nn.Parameter(torch.Tensor(input_size, hidden_size))\n",
        "        self.output_xt = nn.Parameter(torch.Tensor(input_size, hidden_size))\n",
        "        self.output_bias = nn.Parameter(torch.Tensor(hidden_size))\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
        "        for weight in self.parameters():\n",
        "            nn.init.uniform_(weight, -stdv, stdv)\n",
        "\n",
        "    def forward(self, input, hidden_states):\n",
        "        hidden_seq = []\n",
        "        seq_size, batch_size, _ = input.size()\n",
        "\n",
        "        if hidden_states is None:\n",
        "            hidden_states = torch.zeros(\n",
        "                batch_size, self.hidden_size).to(input.device)\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "        for t in range(seq_size):\n",
        "            x_t = input[t, :, :]\n",
        "\n",
        "            rt = torch.sigmoid(x_t @ self.reset_xt +\n",
        "                               hidden_states @ self.reset_ht)\n",
        "\n",
        "            zt = torch.sigmoid(x_t @ self.update_xt +\n",
        "                               hidden_states @ self.update_ht)\n",
        "\n",
        "            qt = rt * hidden_states\n",
        "\n",
        "            ht_1 = torch.tanh(x_t @ self.output_xt +\n",
        "                              qt @ self.output_qt)\n",
        "\n",
        "            ht_2 = (1-zt) * hidden_states\n",
        "\n",
        "            ht_3 = zt * ht_1\n",
        "\n",
        "            ht = ht_2 + ht_3\n",
        "\n",
        "            hidden_seq.append(ht.unsqueeze(0))\n",
        "\n",
        "        hidden_seq = torch.cat(hidden_seq, dim=0)\n",
        "        hidden_seq = hidden_seq.transpose(0, 1).contiguous()\n",
        "\n",
        "        return hidden_seq, ht\n"
      ],
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrgxGsM-aDgH"
      },
      "source": [
        "LSTM Cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNYf5QQFaDAh"
      },
      "source": [
        "class LSTMCell(nn.Module):\n",
        "    def __init__(self, input_sz: int, hidden_sz: int):\n",
        "        super().__init__()\n",
        "        self.input_sz = input_sz\n",
        "        self.hidden_sz = hidden_sz\n",
        "\n",
        "        self.Ui = nn.Parameter(torch.Tensor(input_sz, hidden_sz))\n",
        "        self.Vi = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
        "        self.Bi = nn.Parameter(torch.Tensor(hidden_sz))\n",
        "\n",
        "        ##################################################################################\n",
        "\n",
        "        self.Uf = nn.Parameter(torch.Tensor(input_sz, hidden_sz))\n",
        "        self.Vf = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
        "        self.Bf = nn.Parameter(torch.Tensor(hidden_sz))\n",
        "\n",
        "        self.Uc = nn.Parameter(torch.Tensor(input_sz, hidden_sz))\n",
        "        self.Vc = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
        "        self.Bc = nn.Parameter(torch.Tensor(hidden_sz))\n",
        "\n",
        "        ##################################################################################\n",
        "\n",
        "        self.Uo = nn.Parameter(torch.Tensor(input_sz, hidden_sz))\n",
        "        self.Vo = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
        "        self.Bo = nn.Parameter(torch.Tensor(hidden_sz))\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        stdv = 1.0 / math.sqrt(self.hidden_sz)\n",
        "        for weight in self.parameters():\n",
        "            weight.data.uniform_(-stdv, stdv)\n",
        "    \n",
        "    def forward(self, x, init_states=None):\n",
        "        hidden_seq = []\n",
        "        sequence_sz, batch_sz, _ = x.size()\n",
        "\n",
        "        if init_states is None:\n",
        "            Ht, Ct = (\n",
        "                torch.zeros(batch_sz, self.hidden_sz).to(x.device),\n",
        "                torch.zeros(batch_sz, self.hidden_sz).to(x.device)\n",
        "            )\n",
        "\n",
        "        else:\n",
        "            Ht, Ct = init_states\n",
        "\n",
        "        for t in range(sequence_sz):\n",
        "            Xt = x[t, :, :]\n",
        "            \n",
        "            # Math inside the cell:\n",
        "\n",
        "            It = torch.sigmoid(Xt @ self.Ui + Ht @ self.Vi + self.Bi)\n",
        "\n",
        "            Ft = torch.sigmoid(Xt @ self.Uf + Ht @ self.Vf + self.Bf)\n",
        "\n",
        "            Gt = torch.tanh(Xt @ self.Uc + Ht @ self.Vc + self.Bc)\n",
        "\n",
        "            Ot = torch.sigmoid(Xt @ self.Uo + Ht @ self.Vo + self.Bo)\n",
        "\n",
        "            Ct = Ft * Ct + It * Gt\n",
        "\n",
        "            Ht = Ot * torch.tanh(Ct)\n",
        "\n",
        "            hidden_seq.append(Ht.unsqueeze(0))\n",
        "\n",
        "        hidden_seq = torch.cat(hidden_seq, dim=0)\n",
        "        hidden_seq = hidden_seq.transpose(0,1).contiguous()\n",
        "        return hidden_seq, (Ht, Ct)\n"
      ],
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIMx-FH823LO"
      },
      "source": [
        "Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uokjB9kj2UPr"
      },
      "source": [
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, ntoken, ninp, nhid, nlayers, dropout=0.5):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.nlayers = nlayers\n",
        "        self.nhid = nhid\n",
        "        self.ntoken = ntoken\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.encoder = nn.Embedding(ntoken, ninp) # Token2Embeddings\n",
        "        self.rnn1 = LSTMCell(ninp, nhid)\n",
        "        self.rnn2 = LSTMCell(ninp, nhid)\n",
        "        self.fc = nn.Linear(nhid, 5000)\n",
        "        self.decoder = nn.Linear(5000, ntoken)\n",
        "        # self.decoder = nn.Linear(nhid, ntoken) # Originally, it was like this.\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.05\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.fill_(0)\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "\n",
        "        emb = self.drop(self.encoder(input))\n",
        "\n",
        "        output, hidden = self.rnn1(emb, hidden)\n",
        "        output = self.drop(output)\n",
        "\n",
        "        output, hidden = self.rnn2(output[0, :, :, :], hidden)\n",
        "        output = self.drop(output)\n",
        "\n",
        "        output = output[1, :, :, :]\n",
        "\n",
        "        output = self.fc(output)\n",
        "\n",
        "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
        "\n",
        "        return F.log_softmax(decoded, dim=1), hidden\n",
        "\n",
        "    def init_hidden(self, bsz):\n",
        "\n",
        "        weight = next(self.parameters()).data\n",
        "        return weight.new_zeros(self.nlayers, bsz, self.nhid), weight.new_zeros(self.nlayers, bsz, self.nhid)"
      ],
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XflWGwc3alv"
      },
      "source": [
        "Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4i_M8k9Z3bYT",
        "outputId": "83eaca2b-0fad-4357-a877-7984036af4bf"
      },
      "source": [
        "data = \"./drive/MyDrive/input\"\n",
        "checkpoint = \"\"\n",
        "interval = 200\n",
        "\n",
        "# Network parameters:\n",
        "emsize = 650\n",
        "nhid = 650\n",
        "nlayers = 2\n",
        "lr = 0.001\n",
        "clip = 0.35\n",
        "epochs = 64\n",
        "batch_size = 64\n",
        "eval_batch_size = 1\n",
        "bptt = 32\n",
        "dropout = 0.5\n",
        "\n",
        "save = './drive/MyDrive/input/output/model_test.pt'\n",
        "\n",
        "torch.manual_seed(1111)\n",
        "\n",
        "def batchify(data, bsz):\n",
        "    nbatch = data.size(0) // bsz\n",
        "    data = data.narrow(0, 0, nbatch * bsz)\n",
        "    data = data.view(bsz, -1).t().contiguous()\n",
        "    return data.to('cuda')\n",
        "    \n",
        "# Load data\n",
        "corpus = Corpus(data)\n",
        "\n",
        "train_data = corpus.train\n",
        "val_data = corpus.valid\n",
        "test_data = corpus.test\n",
        "\n",
        "# Build the model\n",
        "ntokens = len(corpus.dictionary) # Around 10000 words\n",
        "model = RNNModel(ntokens, emsize, nhid, nlayers, dropout).to('cuda')\n",
        "\n",
        "# Criteria\n",
        "opt = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.99))\n",
        "criterion = loss = nn.NLLLoss(ignore_index = 0)\n",
        "best_val_loss = None\n",
        "\n",
        "# Load checkpoint\n",
        "if checkpoint != '':\n",
        "    model = torch.load(checkpoint, map_location=lambda storage, loc: storage)\n",
        "\n",
        "print(model)\n",
        "\n",
        "############################################################\n",
        "\n",
        "# def repackage_hidden(h): # For GRU Implementation\n",
        "#     # detach\n",
        "#     return h.clone().detach()\n",
        "\n",
        "def repackage_hidden(h): # For LSTM Implementation\n",
        "    return tuple(e.detach() for e in h)\n",
        "\n",
        "def get_batch(source, i, batch_size):\n",
        "    maxLen = 0\n",
        "    data = []\n",
        "    target = []\n",
        "    for sentence in source[batch_size * i: batch_size * (i+1)]:\n",
        "        data.append(sentence[:-1])\n",
        "        target.append(sentence[1:])\n",
        "\n",
        "    pad_data = pad_sequence(data, padding_value = 0) # 0 is '<pad>'\n",
        "    pad_target = pad_sequence(target, padding_value = 0)\n",
        "    \n",
        "    return pad_data, pad_target \n",
        "\n",
        "def evaluate(data_source):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        total_loss = 0\n",
        "        ntokens = len(corpus.dictionary)\n",
        "        hidden = model.init_hidden(eval_batch_size)\n",
        "        counter = 0\n",
        "\n",
        "        for bindex in range(math.floor(len(data_source) / eval_batch_size)):\n",
        "            data, target = get_batch(data_source, bindex, eval_batch_size)\n",
        "            counter += data.shape[1]\n",
        "            output, hidden = model(data, hidden)\n",
        "            total_loss += criterion(output, target.view(-1)).data\n",
        "            hidden = repackage_hidden(hidden)\n",
        "\n",
        "        return total_loss / counter\n",
        "\n",
        "############################################################\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    start_time = time.time()\n",
        "    hidden = model.init_hidden(batch_size)\n",
        "\n",
        "    for bindex in range(int(len(train_data) / batch_size)):\n",
        "        data, target = get_batch(train_data, bindex, batch_size)\n",
        "        hidden = repackage_hidden(hidden)\n",
        "        output, hidden = model(data, hidden)\n",
        "        loss = criterion(output, target.view(-1))\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        opt.step()\n",
        "\n",
        "        total_loss += loss.data\n",
        "\n",
        "        if bindex % interval == 0 and bindex > 0:\n",
        "            cur_loss = total_loss / interval\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n",
        "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "                epoch,\n",
        "                bindex,\n",
        "                len(train_data) // batch_size,\n",
        "                lr,\n",
        "                elapsed * 1000 / interval,\n",
        "                cur_loss,\n",
        "                math.exp(cur_loss)\n",
        "                )\n",
        "              )\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "\n",
        "try:\n",
        "\n",
        "    for epoch in range(1, epochs+1):\n",
        "        epoch_start_time = time.time()\n",
        "        train()\n",
        "        val_loss = evaluate(val_data)\n",
        "        print('-' * 89)\n",
        "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "                'valid ppl {:8.2f}'.format(epoch,\n",
        "                                           (time.time() - epoch_start_time),\n",
        "                                           val_loss,\n",
        "                                           math.exp(val_loss)\n",
        "                                           )\n",
        "                )\n",
        "        print('-' * 89)\n",
        "\n",
        "        if not best_val_loss or val_loss < best_val_loss:\n",
        "            with open(save, 'wb') as f:\n",
        "                torch.save(model, f)\n",
        "            best_val_loss = val_loss\n",
        "\n",
        "############################################################\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print('-' * 89)\n",
        "    print('Exiting from training early')\n",
        "\n",
        "with open(save, 'rb') as f:\n",
        "    model = torch.load(f)\n",
        "\n",
        "test_loss = evaluate(test_data)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
        "    test_loss,\n",
        "    math.exp(test_loss)\n",
        "    )\n",
        ")\n",
        "print('=' * 89)\n"
      ],
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RNNModel(\n",
            "  (drop): Dropout(p=0.5, inplace=False)\n",
            "  (encoder): Embedding(10001, 650)\n",
            "  (rnn1): LSTMCell()\n",
            "  (rnn2): LSTMCell()\n",
            "  (fc): Linear(in_features=650, out_features=5000, bias=True)\n",
            "  (decoder): Linear(in_features=5000, out_features=10001, bias=True)\n",
            ")\n",
            "=========================================================================================\n",
            "| End of training | test loss  4.34 | test ppl    77.04\n",
            "=========================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}